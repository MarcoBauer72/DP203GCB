{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring and fixing data with Synapse Spark\r\n",
        "\r\n",
        "In this task, you will use a Synapse Spark notebook to explore a few of the files in the **wwi-02/sale-poc** folder in the data lake. You will also use Python code to fix the issues with the **sale-20170502.csv** file.\r\n",
        "\r\n",
        "1. First, attach this notebook to the **SparkPool01** Spark pool.\r\n",
        "2. In the code cell below, replace **asadatalake*SUFFIX*** `with the name of the primary data lake storage account associated with your Syanpse workspace. Then execute the cell by selecting the **Run cell** button that becomes visible when you select the cell.\r\n",
        "\r\n",
        "> **Note**: The cell may take some time to run because the spark cluster must be started."
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "adls_account_name = 'asadatalakeSUFFIX'"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring files with Spark\r\n",
        "\r\n",
        "1. The first step in exploring data using Synapse Spark is to load a file from the data lake. For this, we'll use the **spark.read.load()** method of the **SparkSession** to load the **sale-20170501.csv** file into a [DataFrame](https://spark.apache.org/docs/2.2.0/sql-programming-guide.html#datasets-and-dataframes).\r\n"
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# First, load the file `sale-20170501.csv` file, which we know from our previous exploration to be formatted correctly.\r\n",
        "# Note the use of the `header` and `inferSchema` parameters. Header indicates the first row of the file contains column headers,\r\n",
        "# and `inferSchema` instruct Spark to use data within the file to infer data types.\r\n",
        "df = spark.read.load(f'abfss://wwi-02@{adls_account_name}.dfs.core.windows.net/sale-poc/sale-20170501.csv', format='csv', header=True, inferSchema=True)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View the contents of the DataFrame\r\n",
        "\r\n",
        "With the data from the **sale-20170501.csv** file loaded into a data frame, we can now use various methods of a data frame to explore the properties of the data.\r\n",
        "\r\n",
        "1. Let's look at the data as it was imported. Execute the cell below to view and inspect the data in the data frame."
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "display(df.limit(10))"
      ],
      "outputs": [],
      "metadata": {
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "TransactionId"
            ],
            "values": [
              "CustomerId"
            ],
            "yLabel": "CustomerId",
            "xLabel": "TransactionId",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": "{\"CustomerId\":{\"cdd2ed88-8aae-4295-884a-ac4d40c3c33c\":44,\"e067fc11-e07d-4517-bc93-f7dc4b44f35e\":18}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Like we saw during exploration with the SQL on-demand capabilities of Azure Synapse, Spark allows us to view and query against the data contained within files. \r\n",
        "\r\n",
        "3. Now, use the **printSchema()** method of the data frame to view the results of using the **inferSchema** parameter when creating the data frame. Execute the cell below and observe the output."
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Now, print the inferred schema. We will need this information below to help with the missing headers in the May 2, 2017 file.\r\n",
        "df.printSchema()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. The **printSchema** method outputs both field names and data types that are based on the Spark engine's evaluation of the data contained within each field.\r\n",
        "\r\n",
        "    > We can use this information later to help define the schema for the poorly formed **sale-20170502.csv** file. In addition to the field names and data types, we should note the number of features or columns contained in the file. In this case, note that there are 11 fields. That will be used to determine where to split the single row of data.\r\n",
        "\r\n",
        "5. As an example of further exploration we can do, run the cell below to create and display a new data frame that contains an ordered list of distinct Customer and Product Id pairings. We can use these types of functions to find invalid or empty values quickly in targeted fields."
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Create a new data frame containing a list of distinct CustomerId and ProductId values in descending order of the CustomerId.\r\n",
        "df_distinct_products = df.select('CustomerId', 'ProductId').distinct().orderBy('CustomerId')\r\n",
        "\r\n",
        "# Display the first 100 rows of the resulting data frame.\r\n",
        "display(df_distinct_products.limit(100))"
      ],
      "outputs": [],
      "metadata": {
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [],
            "values": [
              "ProductId"
            ],
            "yLabel": "ProductId",
            "xLabel": "",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": "{\"ProductId\":{\"\":189206}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Next, let's attempt to open and explore the **sale-20170502.csv** file using the **load()** method, as we did above."
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Next, let's try to read in the May 2, 2017 file using the same `load()` method we used for the first file.\r\n",
        "df = spark.read.load(f'abfss://wwi-02@{adls_account_name}.dfs.core.windows.net/sale-poc/sale-20170502.csv', format='csv')\r\n",
        "display(df.limit(10))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. As we saw in T-SQL, we receive a similar error in Spark that the number of columns processed may have exceeded limit of 20480 columns. To work with the data in this file, we need to use more advanced methods, as you will see in the next section below.\n"
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling and fixing poorly formed CSV files\r\n",
        "\r\n",
        "> The steps below provide example code for fixing the poorly-formed CSV file, **sale-20170502.csv** we discovered during exploration of the files in the **wwi-02/sale-poc** folder. This is just one of many ways to handle \"fixing\" a poorly-formed CSV file using Spark.\r\n",
        "\r\n",
        "1. To \"fix\" the bad file, we need to take a programmatic approach, using Python to read in the contents of the file and then parse them to put them into the proper shape.\r\n",
        "\r\n",
        "    > To handle the data being in a single row, we can use the **textFile()** method of our **SparkContext** to read the file as a collection of rows into a resilient distributed dataset (RDD). This allows us to get around the errors around the number of columns because we are essentially getting a single string value stored in a single column.\r\n",
        "\r\n",
        "2. Execute the cell below to load the RDD with data from the file."
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Import the NumPy library. NumPy is a python library used for working with arrays.\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Read the CSV file into a resilient distributed dataset (RDD) as a text file. This will read each row of the file into rows in an RDD.\r\n",
        "rdd = sc.textFile(f'abfss://wwi-02@{adls_account_name}.dfs.core.windows.net/sale-poc/sale-20170502.csv')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. With the data now stored in an RDD, we can access the first, and only, populated row in the RDD, and split that into individual fields. We know from our inspection of the file in Notepad++ that it all the fields are separated by a comma (,), so let's start by splitting on that to create an array of field values. Execute the cell below to create a data array."
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Since we know there is only one row, grab the first row of the RDD and split in on the field delimiter (comma).\r\n",
        "data = rdd.first().split(',')\r\n",
        "\r\n",
        "field_count = len(data)\r\n",
        "# Print out the count of fields read into the array.\r\n",
        "print(field_count)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. By splitting the row on the field delimiter, we created an array of all the individual field values in the file, the count of which you can see above.\n",
        "\n",
        "5. Now, run the cell below to do a quick calculation on the expected number of rows that will be generated by parsing every 11 fields into a single row."
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import math\r\n",
        "\r\n",
        "expected_row_count = math.floor(field_count / 11)\r\n",
        "print(f'The expected row count is: {expected_row_count}')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Next, let's create an array to store the data associated with each \"row\".\r\n",
        "\r\n",
        "    > We will set the max_index to the number of columns that are expected in each row. We know from our exploration of other files in the **wwi-02/sale-poc** folder that they contain 11 columns, so that is the value we will set.\r\n",
        "\r\n",
        "7. In addition to setting variables, we will use the cell below to loop through the **data** array and assign every 11 values to a row. By doing this, we are able to \"split\" the data that was once a single row into appropriate rows containing the proper data and columns from the file.\r\n",
        "\r\n",
        "8. Execute the cell below to create an array of rows from the file data."
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Create an array to store the data associated with each \"row\". Set the max_index to the number of columns that are in each row. This is 11, which we noted above when viewing the schema of the May 1 file.\r\n",
        "row_list = []\r\n",
        "max_index = 11\r\n",
        "\r\n",
        "# Now, we are going to loop through the array of values extracted from the single row of the file and build rows consisting of 11 columns.\r\n",
        "while max_index <= len(data):\r\n",
        "    row = [data[i] for i in np.arange(max_index-11, max_index)]\r\n",
        "    row_list.append(row)\r\n",
        "\r\n",
        "    max_index += 11\r\n",
        "\r\n",
        "print(f'The row array contains {len(row_list)} rows. The expected number of rows was {expected_row_count}.')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. The last thing we need to do to be able to work with the file data as rows is to read it into a Spark DataFrame. In the cell below, we use the **createDataFrame()** method to convert the **row_list** array into a data frame, which also adding names for the columns. Column names are based on the schema we observed in the well formatted files in the **wwi-02/sale-poc** directory.\r\n",
        "\r\n",
        "10. Execute the cell below to create a data frame containing row data from the file and then display the first 10 rows."
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Finally, we can use the row_list we created above to create a DataFrame. We can add to this a schema parameter, which contains the column names we saw in the schema of the first file.\r\n",
        "df_fixed = spark.createDataFrame(row_list,schema=['TransactionId', 'CustomerId', 'ProductId', 'Quantity', 'Price', 'TotalAmount', 'TransactionDateId', 'ProfitAmount', 'Hour', 'Minute', 'StoreId'])\r\n",
        "display(df_fixed.limit(10))"
      ],
      "outputs": [],
      "metadata": {
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "TransactionId"
            ],
            "values": [
              "TransactionId"
            ],
            "yLabel": "TransactionId",
            "xLabel": "TransactionId",
            "aggregation": "COUNT",
            "aggByBackend": false
          },
          "aggData": "{\"TransactionId\":{\"5455a4b4-62bd-401a-b5c6-79ea24f30531\":5,\"a4116581-5aad-416a-b767-aefa516737b1\":5}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write the \"fixed\" file into the data lake\r\n",
        "\r\n",
        "1. The last step we will take as part of our exploration and file fixing process is to write the data back into the data lake, so it can be ingested following the same process as the other files in the **wwi-02/sale-poc** folder.\r\n",
        "\r\n",
        "2. Execute the cell below to save the data frame into the data lake a series of files in a folder named **sale-20170502-fixed**.\r\n",
        "\r\n",
        "    > Note: Spark parallelizes workloads across worker nodes, so when saving files, you will notice they are saved as a collection \"part\" files, and not as a single file. While there are some libraries you can use to create a single file, it is helpful to get used to working with files generated via Spark notebooks as they are natively created.\r\n"
      ],
      "metadata": {},
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df_fixed.write.format('csv').option('header',True).mode('overwrite').option('sep',',').save(f'abfss://wwi-02@{adls_account_name}.dfs.core.windows.net/sale-poc/sale-20170502-fixed')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect the fixed file in the data lake\r\n",
        "\r\n",
        "1. With the fixed file written to the data lake, you can quickly inpsect it to verify the files are now formatted properly. Select the **wwi-02** tab above to view the **sale-poc** folder.\r\n",
        "2. Refresh the folder view (expand the **More** menu if necessary) and then open the **sale-20170502-fixed** folder.\r\n",
        "3. In the **sale-20170502-fixed** folder, right-click the first file whose name begins with **part** and whose extension is **.csv** and select **Preview** from the context menu.\r\n",
        "4. In the **Preview** dialog, verify you see the proper columns and that the data looks valid in each field.\r\n",
        "\r\n",
        "## Wrap-up\r\n",
        "\r\n",
        "Throughout this exercise, you used a Spark notebook to explore data stored within files in the data lake. You used Python code to extract data from a poorly formatted CSV file, assemble the data from that file into proper rows, and then write the \"fixed\" file back out into your data lake.\r\n",
        "\r\n",
        "You can now return to the lab guide to continue with the next section of Lab 2.\r\n"
      ],
      "metadata": {},
      "attachments": {}
    }
  ]
}
